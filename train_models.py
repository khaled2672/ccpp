# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OuOCIlmcWtpBciN-Wi1I0Wpmt_yLj99T
"""

import pandas as pd
df = pd.read_csv('/content/in/data.csv')
df

from sklearn.preprocessing import MinMaxScaler
MinMaxScaler = MinMaxScaler()
df[['Ambient Relative Humidity', 'Ambient Pressure', 'Ambient Temperature', 'Exhaust Vacuum', 'Total Power']] = MinMaxScaler.fit_transform(
    df[['Ambient Relative Humidity', 'Ambient Pressure', 'Ambient Temperature', 'Exhaust Vacuum', 'Total Power']]
)
target = df['Total Power']
features = df.drop(columns=['Total Power'])
print("Features:\n", features.head())
print("Target:\n", target.head())

import seaborn as sns
import matplotlib.pyplot as plt

correlation_matrix = df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Matrix")
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Load data
df = pd.read_csv("/content/in/data.csv")  # Update path if needed
X = df.drop("Total Power", axis=1)
y = df["Total Power"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# StandardScaler for model training
standard_scaler = StandardScaler()
X_train_scaled = standard_scaler.fit_transform(X_train)
X_test_scaled = standard_scaler.transform(X_test)

# Define models
models = {
    "Random Forest": RandomForestRegressor(
        n_estimators=100,
        max_depth=15,
        min_samples_split=2,
        min_samples_leaf=1,
        random_state=42
    ),
    "XGBoost": XGBRegressor(
        n_estimators=300,
        max_depth=9,
        learning_rate=0.2,
        subsample=0.9,
        random_state=50,
        verbosity=0
    ),
}

tuned_models = {}
results = {}

# Train and evaluate
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train_scaled, y_train)
    tuned_models[name] = model

    y_pred = model.predict(X_test_scaled)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mae = np.mean(np.abs(y_test - y_pred))
    mbe = np.mean(y_test - y_pred)

    results[name] = {"MSE": mse, "R2": r2, "MAE": mae, "MBE": mbe}
    print(f"{name} - R2: {r2:.4f}, MSE: {mse:.4f}, MAE: {mae:.4f}, MBE: {mbe:.4f}")

# Ensemble optimization
rf_preds = tuned_models["Random Forest"].predict(X_test_scaled)
xgb_preds = tuned_models["XGBoost"].predict(X_test_scaled)

best_r2 = -np.inf
best_w = 0.0
for w in np.arange(0, 1.05, 0.05):
    blended = w * rf_preds + (1 - w) * xgb_preds
    score = r2_score(y_test, blended)
    if score > best_r2:
        best_r2 = score
        best_w = w

final_preds = best_w * rf_preds + (1 - best_w) * xgb_preds
combined_mse = mean_squared_error(y_test, final_preds)
combined_r2 = r2_score(y_test, final_preds)
combined_mae = np.mean(np.abs(y_test - final_preds))
combined_mbe = np.mean(y_test - final_preds)

results["Combined"] = {
    "MSE": combined_mse, "R2": combined_r2, "MAE": combined_mae, "MBE": combined_mbe
}

print(f"\n‚úÖ Best Ensemble Weight: {best_w:.2f} RF / {1 - best_w:.2f} XGB")
print(f"Combined R2: {combined_r2:.4f}, MSE: {combined_mse:.4f}, MAE: {combined_mae:.4f}, MBE: {combined_mbe:.4f}")

# Save models and scalers
joblib.dump(tuned_models["Random Forest"], "rf_model.pkl", compress=9)
joblib.dump(tuned_models["XGBoost"], "xgb_model.pkl", compress=9)
joblib.dump(standard_scaler, "scaler.pkl", compress=3)
joblib.dump(X.columns.tolist(), "features.pkl")

# Save the best ensemble weight
with open("best_weight.txt", "w") as f:
    f.write(str(best_w))

print("‚úÖ Models, scaler, and best weight saved.")

# Plot results
metrics = ['R2', 'MSE', 'MAE', 'MBE']
model_names = list(results.keys())

fig, axes = plt.subplots(1, 4, figsize=(20, 6))
for i, metric in enumerate(metrics):
    values = [results[m][metric] for m in model_names]
    axes[i].bar(model_names, values, color=['#4169E1', '#FF8C00', '#2E8B57'])
    axes[i].set_title(f"{metric} Comparison")
    axes[i].set_ylabel(metric)
    axes[i].tick_params(axis='x', rotation=30)

plt.tight_layout()
plt.show()

try:
    with open("best_weight.txt", "r") as f:
        best_w = float(f.read().strip())
except FileNotFoundError:
    st.warning("‚ö†Ô∏è best_weight.txt not found. Using default ensemble weight.")
    best_w = 0.5  # Default fallback

def ensemble_predict(features):
    """Predict power using the weighted ensemble of RF and XGBoost."""
    rf_pred = tuned_models["Random Forest"].predict(features)
    xgb_pred = tuned_models["XGBoost"].predict(features)
    return best_w * rf_pred + (1 - best_w) * xgb_pred
with open("requirements.txt", "w") as f:
    f.write("streamlit\nnumpy\nscikit-learn\nxgboost\njoblib\n")

!pip install pyswarms

import numpy as np
import pandas as pd
import os
from pyswarms.single.global_best import GlobalBestPSO
from sklearn.preprocessing import MinMaxScaler

# 1Ô∏è‚É£ Define the features used for optimization
selected_features = ['Ambient Temperature', 'Ambient Relative Humidity', 'Ambient Pressure', 'Exhaust Vacuum']
import joblib


# Save the feature column names in order

# 5Ô∏è‚É£ PSO objective function that also optimizes ensemble blending weight
def objective_function(x):
    preds = []
    for i in range(x.shape[0]):
        input_features = x[i, :-1]  # Last element is the ensemble weight
        w = np.clip(x[i, -1], 0, 1)

        # Scale features
        input_scaled = scaler.transform(input_features.reshape(1, -1))

        # Get predictions
        rf_pred = tuned_models["Random Forest"].predict(input_scaled)
        xgb_pred = tuned_models["XGBoost"].predict(input_scaled)

        # Weighted prediction
        ensemble_pred = w * rf_pred + (1 - w) * xgb_pred

        # PSO minimizes, so we negate the target to maximize
        preds.append(-ensemble_pred)
    return np.ravel(preds)

# 6Ô∏è‚É£ Define bounds for each feature and for the ensemble weight
feature_bounds = {
    'Ambient Temperature': [16.788, 37.101],
    'Ambient Relative Humidity': [20.244555, 88.487236],
    'Ambient Pressure': [797.8021, 800.0850],
    'Exhaust Vacuum': [3.000248, 12.000992],
}

lb = [feature_bounds[feat][0] for feat in selected_features] + [0.0]  # + blending weight lower bound
ub = [feature_bounds[feat][1] for feat in selected_features] + [1.0]  # + blending weight upper bound
bounds = (lb, ub)

# 7Ô∏è‚É£ Run the optimizer
optimizer = GlobalBestPSO(
    n_particles=50,
    dimensions=5,  # 4 features + 1 weight
    options={'c1': 0.5, 'c2': 0.3, 'w': 0.9},
    bounds=bounds
)

cost, pos = optimizer.optimize(objective_function, iters=100)

# 8Ô∏è‚É£ Extract results
optimal_input = pos[:-1]
optimal_weight = pos[-1]

# Scale and predict final ensemble output
scaled_input = scaler.transform(np.array(optimal_input).reshape(1, -1))
rf_pred = tuned_models["Random Forest"].predict(scaled_input)
xgb_pred = tuned_models["XGBoost"].predict(scaled_input)
final_power = optimal_weight * rf_pred + (1 - optimal_weight) * xgb_pred

# 9Ô∏è‚É£ Display output
print("\n‚úÖ Optimal Input Features (original scale):")
for name, val in zip(selected_features, optimal_input):
    print(f"{name}: {val:.4f}")

print(f"\n‚öñÔ∏è Optimized Ensemble Weight: {optimal_weight:.2f} RF / {1 - optimal_weight:.2f} XGB")
print(f"\n‚ö° Max Predicted Power (via optimized input): {final_power[0]:.4f} MW")

pip install streamlit

import streamlit as st
import numpy as np

st.title("üîå Power Prediction App")

# Input sliders
temp = st.slider("Ambient Temperature (¬∞C)", 15.0, 40.0, 25.78)
humidity = st.slider("Ambient Relative Humidity (%)", 20.0, 100.0, 60.17)
pressure = st.slider("Ambient Pressure (mmHg)", 795.0, 805.0, 798.98)
vacuum = st.slider("Exhaust Vacuum (inHg)", 3.0, 12.0, 10.43)

# Predict
input_data = np.array([[temp, humidity, pressure, vacuum]])
scaled = scaler.transform(input_data)
pred_power = ensemble_predict(scaled)[0]

st.metric(label="‚ö° Predicted Power Output (MW)", value=f"{pred_power:.4f}")

st.write("‚úÖ Ordered Input DataFrame", df)

import joblib
from sklearn.preprocessing import StandardScaler  # or whichever you used

# Assuming your scaler is called 'scaler'
joblib.dump(scaler, 'scaler.pkl', compress=3)

pip install -r requirements.txt

pip show streamlit